{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Exercise 3: Seq2Seq Model and Attention Mechanisms\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Encoder and Decoder for Seq2Seq without Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        ouputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.embedding(input)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq Model without Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        trg_len, batch_size = trg.shape\n",
    "        trg_vocab_size = self.decoder.fc_out.out_features\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size)\n",
    "        \n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = trg[0, :]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Attention Mechanism and Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_dim))\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        hidden = hidden[-1].repeat(src_len, 1, 1).transpose(0, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention = torch.sum(self.v * energy, dim=2)\n",
    "        return torch.softmax(attention, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, attention):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim + hidden_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        input.unsqueeze(0)\n",
    "        embedded = self.embedding(input)\n",
    "        a = self.attention(hidden, encoder_outputs).unsqueeze(1)\n",
    "        weighted = torch.bmm(a, encoder_outputs.permute(1, 0, 2))\n",
    "        rnn_input = torch.cat((embedded, weighted.permute(1, 0, 2)), dim=2)\n",
    "        output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
    "        prediction = self.fc_out(torch.cat((output.squeeze(0), weighted.squeeze(0)), dim=1))\n",
    "        return prediction, hidden, cell, a.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Seq2Seq Model with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqWithAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2SeqWithAttention, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        trg_len, batch_size = trg.shape\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size)\n",
    "        attentions = torch.zeros(trg_len, src.shape[0], batch_size)\n",
    "        \n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src)\n",
    "        input = trg[0, :]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell, attention = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            attentions[t] = attention\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs, attentions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for Seq2Seq without attention: 1.0547686614863434e-154\n",
      "BLEU score for Seq2Seq with attention: 1.0\n"
     ]
    }
   ],
   "source": [
    "def calculate_bleu_score(reference, candidate):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for a single reference and candidate sentence pair.\n",
    "    \n",
    "        :param reference: List of words in the target sentence (ground truth).\n",
    "        :param candidate: List of words in the predicted sentence.\n",
    "\n",
    "    Return: BLEU score (float)\n",
    "    \"\"\"\n",
    "    return sentence_bleu([reference], candidate)\n",
    "\n",
    "# Example usage\n",
    "reference = \"I am learning NLP\".split()\n",
    "candidate_seq2seq = \"I am learni NLP\".split()  # Example output without attention\n",
    "candidate_with_attention = \"I am learning NLP\".split()  # Example output with attention\n",
    "\n",
    "bleu_seq2seq = calculate_bleu_score(reference, candidate_seq2seq)\n",
    "bleu_with_attention = calculate_bleu_score(reference, candidate_with_attention)\n",
    "\n",
    "print(f\"BLEU score for Seq2Seq without attention: {bleu_seq2seq}\")\n",
    "print(f\"BLEU score for Seq2Seq with attention: {bleu_with_attention}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
