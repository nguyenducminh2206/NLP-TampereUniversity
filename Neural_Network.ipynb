{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Exercise 2: Neural Network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going from raw text to tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install datasets\n",
    "%pip install tokenizers\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Datasets from Hugging Face could be useful when we create our own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(raw_sentences: list[str]) -> datasets.Dataset:\n",
    "    \"\"\"\n",
    "    Create a HuggingFace Dataset.\n",
    "    \n",
    "    Parameters: \n",
    "        raw_sentences: list of sentences.\n",
    "        labels: list of integer labels corresponding to the sentences.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_dict = {\n",
    "        \"text\": raw_sentences,\n",
    "\n",
    "    }\n",
    "\n",
    "    # Define schema\n",
    "    dataset_features = datasets.Features(\n",
    "        {\n",
    "            \"text\": datasets.Value(\"string\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create the datset\n",
    "    dataset = datasets.Dataset.from_dict(dataset_dict, features=dataset_features)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                                  I am studying NLP\n",
      "1  I am living in Finland and I love walking thro...\n",
      "2                The weather is getting cold. Great!\n"
     ]
    }
   ],
   "source": [
    "raw_sentences = [\"I am studying NLP\", \n",
    "                 \"I am living in Finland and I love walking through the forests.\", \n",
    "                 \"The weather is getting cold. Great!\"]\n",
    "\n",
    "own_dataset = create_dataset(raw_sentences)\n",
    "\n",
    "print(own_dataset.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences between WordPiece tokenization and wordLevel tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to train WordLevel Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_level_tokenizer(\n",
    "        sentences: list[str],\n",
    "        unk_token: str = \"[UNK]\",\n",
    "        pad_token: str = \"[PAD]\",\n",
    "        start_of_seq_token: str = \"<s>\",\n",
    "        end_of_seq_token: str = \"</s>\", \n",
    "        vocab_size: int = 100\n",
    ") -> PreTrainedTokenizerFast:\n",
    "    \"\"\"Train a WordLevel tokenizer.\"\"\"\n",
    "    special_tokens = [unk_token, pad_token, start_of_seq_token, end_of_seq_token]\n",
    "    trainer = trainers.WordLevelTrainer(vocab_size=vocab_size,\n",
    "                                        special_tokens=special_tokens, \n",
    "                                        show_progress=True)\n",
    "\n",
    "    # Initialize WordLevel tokenizer\n",
    "    tokenizer = Tokenizer(models.WordLevel(unk_token=unk_token))\n",
    "\n",
    "    # Normalize each sentence using NFD unicode and stripping whitespace\n",
    "    tokenizer.normalizer = normalizers.Sequence(\n",
    "        [normalizers.NFD(), normalizers.Strip()]\n",
    "    )\n",
    "\n",
    "    # Using Whitespace to split each input sentence\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "    # Post-process for sequence boundaries\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=f\"{start_of_seq_token} $A {end_of_seq_token}\",\n",
    "        special_tokens=[\n",
    "            (start_of_seq_token, special_tokens.index(start_of_seq_token)),\n",
    "            (end_of_seq_token, special_tokens.index(end_of_seq_token)),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train tokenizer\n",
    "    tokenizer.train_from_iterator(sentences, trainer=trainer)\n",
    "\n",
    "    # Enable padding\n",
    "    tokenizer.enable_padding(pad_id=special_tokens.index(pad_token), pad_token=pad_token)\n",
    "\n",
    "    # Wrap in PreTrainedTokenizerFast\n",
    "    pretrained_tokenizer = PreTrainedTokenizerFast(\n",
    "        bos_token=start_of_seq_token,\n",
    "        eos_token=end_of_seq_token,\n",
    "        unk_token=unk_token,\n",
    "        pad_token=pad_token,\n",
    "        tokenizer_object=tokenizer,\n",
    "    )\n",
    "    return pretrained_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to train WordPiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wordpiece_tokenizer(\n",
    "    sentences: list[str],\n",
    "    unk_token: str = \"[UNK]\",\n",
    "    pad_token: str = \"[PAD]\",\n",
    "    start_of_seq_token: str = \"<s>\",\n",
    "    end_of_seq_token: str = \"</s>\",\n",
    "    vocab_size: int = 100  # Set a smaller vocab size to force subword splits\n",
    ") -> PreTrainedTokenizerFast:\n",
    "    \"\"\"Train a WordPiece tokenizer.\"\"\"\n",
    "    special_tokens = [unk_token, pad_token, start_of_seq_token, end_of_seq_token]\n",
    "    trainer = trainers.WordPieceTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=special_tokens,\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "    # Initialize WordPiece tokenizer\n",
    "    tokenizer = Tokenizer(models.WordPiece(unk_token=unk_token))\n",
    "\n",
    "    # Configure normalization and pre-tokenization\n",
    "    tokenizer.normalizer = normalizers.Sequence([normalizers.NFD(), normalizers.Strip()])\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "    # Post-process for sequence boundaries\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=f\"{start_of_seq_token} $A {end_of_seq_token}\",\n",
    "        special_tokens=[\n",
    "            (start_of_seq_token, special_tokens.index(start_of_seq_token)),\n",
    "            (end_of_seq_token, special_tokens.index(end_of_seq_token)),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train tokenizer\n",
    "    tokenizer.train_from_iterator(sentences, trainer=trainer)\n",
    "\n",
    "    # Enable padding\n",
    "    tokenizer.enable_padding(pad_id=special_tokens.index(pad_token), pad_token=pad_token)\n",
    "\n",
    "    # Wrap in PreTrainedTokenizerFast\n",
    "    pretrained_tokenizer = PreTrainedTokenizerFast(\n",
    "        bos_token=start_of_seq_token,\n",
    "        eos_token=end_of_seq_token,\n",
    "        unk_token=unk_token,\n",
    "        pad_token=pad_token,\n",
    "        tokenizer_object=tokenizer,\n",
    "    )\n",
    "    return pretrained_tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train both tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_level_tokenizer = train_word_level_tokenizer(raw_sentences)\n",
    "wordpiece_tokenizer = train_wordpiece_tokenizer(raw_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I am studying NLP\n",
      "Tokens: ['<s>', 'I', 'am', 'studying', 'NLP', '</s>']\n",
      "Token IDs: [2, 4, 6, 20, 10, 3]\n",
      "Sentence: I am living in Finland and I love walking through the forests.\n",
      "Tokens: ['<s>', 'I', 'am', 'living', 'in', 'Finland', 'and', 'I', 'love', 'walking', 'through', 'the', 'forests', '.', '</s>']\n",
      "Token IDs: [2, 4, 6, 18, 16, 8, 12, 4, 19, 23, 22, 21, 14, 5, 3]\n",
      "Sentence: The weather is getting cold. Great!\n",
      "Tokens: ['<s>', 'The', 'weather', 'is', 'getting', 'cold', '.', 'Great', '!', '</s>']\n",
      "Token IDs: [2, 11, 24, 17, 15, 13, 5, 9, 7, 3]\n"
     ]
    }
   ],
   "source": [
    "# Print tokenized results for raw sentences using convert_ids_to_tokens\n",
    "for sentence in raw_sentences:\n",
    "    input_ids = word_level_tokenizer.encode(sentence)\n",
    "    tokens = word_level_tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print(\"Token IDs:\", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I am studying NLP\n",
      "Tokens: ['<s>', 'I', 'am', 'st', '##ud', '##ying', 'NLP', '</s>']\n",
      "Token IDs: [2, 8, 55, 71, 78, 80, 94, 3]\n",
      "Sentence: I am living in Finland and I love walking through the forests.\n",
      "Tokens: ['<s>', 'I', 'am', 'living', 'in', 'Finland', 'and', 'I', 'love', 'wa', '##lk', '##ing', 'th', '##roug', '##h', 'the', 'fores', '##ts', '.', '</s>']\n",
      "Token IDs: [2, 8, 55, 98, 67, 92, 63, 8, 99, 74, 89, 53, 72, 87, 40, 73, 96, 77, 5, 3]\n",
      "Sentence: The weather is getting cold. Great!\n",
      "Tokens: ['<s>', 'The', 'weat', '##her', 'is', 'gett', '##ing', 'cold', '.', 'Great', '!', '</s>']\n",
      "Token IDs: [2, 62, 75, 91, 68, 97, 53, 95, 5, 93, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "# Print tokenized results for raw sentences using convert_ids_to_tokens\n",
    "for sentence in raw_sentences:\n",
    "    input_ids = wordpiece_tokenizer.encode(sentence)\n",
    "    tokens = wordpiece_tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print(\"Token IDs:\", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-Level Tokens: ['<s>', 'I', '[UNK]', '[UNK]', '[UNK]', 'and', '[UNK]', '[UNK]', '.', '</s>']\n",
      "Word-Level Token IDs: [2, 4, 0, 0, 0, 12, 0, 0, 5, 3]\n",
      "WordPiece Tokens: ['<s>', 'I', 'wa', '##n', '##t', 't', '##o', 'li', '##ve', 'and', 'st', '##ud', '##y', 'h', '##e', '##r', '##e', '.', '</s>']\n",
      "WordPiece Token IDs: [2, 8, 74, 38, 33, 28, 43, 69, 83, 63, 71, 78, 36, 19, 41, 46, 41, 5, 3]\n"
     ]
    }
   ],
   "source": [
    "# Test both tokenizers on a sample sentence\n",
    "sample_text = \"I want to live and study here.\"\n",
    "\n",
    "# Word-Level Tokenizer Encoding\n",
    "word_level_output = word_level_tokenizer(sample_text)\n",
    "print(\"Word-Level Tokens:\", word_level_tokenizer.convert_ids_to_tokens(word_level_output['input_ids']))\n",
    "print(\"Word-Level Token IDs:\", word_level_output['input_ids'])\n",
    "\n",
    "# WordPiece Tokenizer Encoding\n",
    "wordpiece_output = wordpiece_tokenizer(sample_text)\n",
    "print(\"WordPiece Tokens:\", wordpiece_tokenizer.convert_ids_to_tokens(wordpiece_output['input_ids']))\n",
    "print(\"WordPiece Token IDs:\", wordpiece_output['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTSM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the WikiDataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = sentences\n",
    "        self.max_length = max_length\n",
    "        self.masked_sentences = []  \n",
    "        self.original_sentences = [] \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_sentence = self.sentences[idx]\n",
    "        words = original_sentence.split()\n",
    "        mask_index = random.randint(1, len(words) - 2)  # Avoid masking first or last word\n",
    "        target = self.tokenizer.convert_tokens_to_ids(words[mask_index])\n",
    "        words[mask_index] = '[MASK]'\n",
    "        masked_sentence = ' '.join(words)\n",
    "\n",
    "        # Store sentences for display\n",
    "        self.original_sentences.append(original_sentence)\n",
    "        self.masked_sentences.append(masked_sentence)\n",
    "\n",
    "        # Tokenize the masked sentence\n",
    "        inputs = self.tokenizer(masked_sentence, return_tensors='pt', max_length=self.max_length,\n",
    "                                padding='max_length', truncation=True)\n",
    "        \n",
    "        input_ids = inputs['input_ids'].squeeze(0)  # Remove the batch dimension\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "\n",
    "        return input_ids, attention_mask, target\n",
    "\n",
    "    def get_display_sentences(self, index):\n",
    "        return self.original_sentences[index], self.masked_sentences[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(MaskedLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        mask_out = lstm_out[:, x.size(1) // 2, :]  # assuming the mask is in the middle\n",
    "        logits = self.fc(mask_out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Wiki Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "train_sentences = [line for line in dataset['train']['text'] if len(line.split()) > 5]\n",
    "\n",
    "train_dataset = WikiDataset(train_sentences, tokenizer)\n",
    "data_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "model = MaskedLSTM(vocab_size, embedding_dim, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), weight_decay=1e-5, lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming model, optimizer, criterion are already defined\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    epoch_loss = 0\n",
    "    for input_ids, attention_mask, targets in tqdm(data_loader, desc=\"Training\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Loss {epoch_loss / len(data_loader):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
