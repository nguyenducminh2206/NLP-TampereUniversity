{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Exercise 1: Sentiment Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.corpus import twitter_samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Traditional Sentiment Analysis approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Preprocessing the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization after normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\vpming\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vpming\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import nltk samples, stopwords\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Read corpus package\n",
    "print(twitter_samples.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<WordListCorpusReader in 'C:\\\\Users\\\\vpming\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords'>\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Process tweet function.\n",
    "    \n",
    "    Input:\n",
    "        tweet: a string containing a tweet.\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet.\n",
    "\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract positive and negative tweets\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "all_tweets = positive_tweets + negative_tweets\n",
    "\n",
    "# Create labels: 1 for positive, 0 for negative\n",
    "positive_labels = [1] * len(positive_tweets)\n",
    "negative_labels = [0] * len(negative_tweets)\n",
    "\n",
    "all_labels = positive_labels + negative_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DataFrame for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(tweets, labels):\n",
    "    \"\"\"\n",
    "    Create DataFrame to visualize the dataset.\n",
    "    \n",
    "    Input:\n",
    "        tweet: a list containing tweet texts.\n",
    "        labels: list containing labels for each tweet (1 for positve, 0 for negative).\n",
    "    Output:\n",
    "        A DataFrame with 2 columns of tweets and labels.\n",
    "\n",
    "    \"\"\"\n",
    "    if len(tweets) != len(labels):\n",
    "        raise ValueError\n",
    "\n",
    "    df = pd.DataFrame({'tweets': tweets, 'labels': labels})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              tweets  labels\n",
      "0  #FollowFriday @France_Inte @PKuchly57 @Milipol...       1\n",
      "1  @Lamb2ja Hey James! How odd :/ Please call our...       1\n",
      "2  @DespiteOfficial we had a listen last night :)...       1\n",
      "3                               @97sides CONGRATS :)       1\n",
      "4  yeaaaah yippppy!!!  my accnt verified rqst has...       1\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame for tweets and labels\n",
    "df = create_dataframe(all_tweets, all_labels)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for the Word2Vec model\n",
    "cleaned_tweets = []\n",
    "for tweet in all_tweets:\n",
    "    cleaned_tweet = process_tweet(tweet)\n",
    "    cleaned_tweets.append(cleaned_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embeddings using Word2Vec model\n",
    "word2vec_model = Word2Vec(cleaned_tweets, vector_size=20,\n",
    "                          window=5, min_count=5, workers=4)\n",
    "\n",
    "word_embeddings = word2vec_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.37327787  0.23462231  0.05815687  0.4243192  -0.16658585 -0.03578897\n",
      "  0.34700608  0.8236061  -0.3567064   0.20356919  0.298717   -0.25387537\n",
      "  0.5562919  -0.05993766  0.32543904  0.13163735  0.7409497  -0.2601367\n",
      " -0.11768366 -0.69812775]\n"
     ]
    }
   ],
   "source": [
    "# Example using word embeddings\n",
    "print(word_embeddings['listen'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tweet embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each tweet consists of multiple words, convert each tweet into a single vector representation by averaging the Word2Vec embeddings of all the words in the tweet. This averaged vector will represent the tweet in a fixed-dimensional space, suitable for input into a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_embedding(tweet_tokens, word2vec_model):\n",
    "    \"\"\"\n",
    "    Generate the embedding for a tweet by averaging word vectors.\n",
    "    \n",
    "    Input: \n",
    "        tweet_tokens: a list of tokens from processed tweet.\n",
    "        word2vec_model: a trained Word2Vec model that contains word embeddings.\n",
    "    Output:\n",
    "        tweet_embedding: a numpy array representing the averaged embedding \n",
    "                        vector for a tweet. The dimension of the array is \n",
    "                        equal to the vector_size of the Word2Vec model.\n",
    "\n",
    "    \"\"\"\n",
    "    tweet_vecs = []\n",
    "\n",
    "    for word in tweet_tokens:\n",
    "        if word in word2vec_model.wv:\n",
    "            tweet_vecs.append(word2vec_model.wv[word])\n",
    "\n",
    "    if len(tweet_vecs) == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "    tweet_embedding = np.mean(tweet_vecs, axis=0)\n",
    "\n",
    "    return tweet_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yeaaah', 'yipppi', 'accnt', 'verifi', 'rqst', 'succeed', 'got', 'blue', 'tick', 'mark', 'fb', 'profil', ':)', '15', 'day']\n",
      "[ 0.35383517  0.22469711  0.05685812  0.3446301  -0.12399756 -0.00763755\n",
      "  0.27155995  0.654489   -0.33020473  0.19128002  0.24953935 -0.15964511\n",
      "  0.48445526 -0.09397851  0.24069586  0.1165326   0.60909647 -0.20767407\n",
      " -0.16720642 -0.53667706]\n"
     ]
    }
   ],
   "source": [
    "# Example for using get_tweet_embeddings function\n",
    "tweet1 = df['tweets'][4]\n",
    "tweet1_cleaned = process_tweet(tweet1)\n",
    "print(tweet1_cleaned)\n",
    "tweet1_embedding = get_tweet_embedding(tweet1, word2vec_model)\n",
    "print(tweet1_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare and split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to a numpy array\n",
    "labels = np.array(all_labels)\n",
    "\n",
    "# Generate embeddings for all tweets in the dataset\n",
    "tweet_embeddings = np.array([get_tweet_embedding(\n",
    "    tweet, word2vec_model) for tweet in cleaned_tweets])\n",
    "\n",
    "# Split the dataset into training and test sets (80% train, 20% test)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    tweet_embeddings, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Classifier using Logistic Regression as a baseline classifier, which works well for binary classification tasks like sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the logistic regression classifier\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.9185\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92       988\n",
      "           1       0.92      0.91      0.92      1012\n",
      "\n",
      "    accuracy                           0.92      2000\n",
      "   macro avg       0.92      0.92      0.92      2000\n",
      "weighted avg       0.92      0.92      0.92      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy: \", accuracy)\n",
    "\n",
    "# Detailed classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example prediction by using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, word2vec_model, clf):\n",
    "    \"\"\"\n",
    "    Predict sentiment of a tweet using a trained Word2Vec model and classifier.\n",
    "    \n",
    "    Input:\n",
    "        tweet: raw text to predict sentiment.\n",
    "        word2vec_model: a trained Word2Vec model containing word embeddings.\n",
    "        clf: a trained classifier for sentiment predictions.\n",
    "    Output:\n",
    "        Returns \"Positive\" if the predicted sentiment is positive, \n",
    "                otherwise returns \"Negative\".\n",
    "                \n",
    "    \"\"\"\n",
    "    processed_tweet = process_tweet(tweet)\n",
    "    tweet_embedding = get_tweet_embedding(processed_tweet, word2vec_model)\n",
    "    prediction = clf.predict([tweet_embedding])\n",
    "\n",
    "    return \"Positive\" if prediction == 1 else \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment:  Positive\n"
     ]
    }
   ],
   "source": [
    "# Example prediction\n",
    "new_tweet = \"I like to study NLP <3\"\n",
    "print(\"Sentiment: \", predict_tweet(new_tweet, word2vec_model, clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Layer Perceptrons (MLP) can enhance the predictive power of sentiment analysis model by allowing it to capture more complex patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline for building a Multi-Layer Perceptrons for Sentiment Analysis\n",
    "\n",
    "#### 1. Data Preparation: \n",
    "- Use the embeddings generated for each tweet as input features for the MLP.\n",
    "\n",
    "#### 2. Model Architecture:\n",
    "- Design a simple MLP with several fully connected layers (dense layers), an activation function (ReLu) for non-linearity, and dropout layers to prevent overfitting.\n",
    "\n",
    "- Use a final ouput layer with a sigmoid activation function for binary classification.\n",
    "\n",
    "#### 3. Training and Evaluation:\n",
    "- Train the MLP on the training set, validate on the test set, and evaluate performance using accuracy and a classification report.\n",
    "\n",
    "#### 4. Hyperparameter Tuning:\n",
    "- Experiment with the number of layers, number of neurons, dropout rates, and learning rate to optimize performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Multi-Layer Perceptron (MLP) class in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentMLP(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, \n",
    "                 hidden_dim1: int, \n",
    "                 hidden_dim2: int, \n",
    "                 dropout: float):\n",
    "        \"\"\"\n",
    "        Initialize the Multi-Layer Perceptrons (MLP) model for sentiment analysis.\n",
    "\n",
    "        Parameters:\n",
    "            input_dim (int): dimension of the input features (tweet embedding size).\n",
    "            hidden_dim1 (int): Number of neurons in the first hidden layer.\n",
    "            hidden_dim2 (int): Number of neurons in the second hidden layer.\n",
    "            dropout_rate (float): Dropout rate to prevent overfitting.\n",
    "\n",
    "        \"\"\"\n",
    "        super(SentimentMLP, self).__init__()\n",
    "\n",
    "        # Using nn.Sequential to stack layers\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim1),  # First hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),  # Second hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim2, 1),  # Output layer\n",
    "            nn.Sigmoid()  # Sigmoid activation for binary classification\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(tweet_embeddings)  # tweet_embeddings generated from Word2Vec\n",
    "y = np.array(labels)  # labels for the tweets (1 for positive, 0 for negative)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Split the data into training and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters for the model\n",
    "embedding_dim = X_train.shape[1]\n",
    "hidden_dim1 = 64\n",
    "hidden_dim2 = 32\n",
    "dropout_rate = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = SentimentMLP(\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim1=hidden_dim1,\n",
    "    hidden_dim2=hidden_dim2,\n",
    "    dropout=dropout_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation for training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, criterion, optimizer, num_epochs, batch_size, print_every):\n",
    "    \"\"\"\n",
    "    Train a model with the given dataset, loss function, and optimizer.\n",
    "\n",
    "    Parameters:\n",
    "        model: the neural network model to train.\n",
    "        x_train: training features.\n",
    "        y_train: training labels.\n",
    "        criterion: loss function.\n",
    "        optimizer: optimizer to update model parameters.\n",
    "        num_epochs: number of epochs to train.\n",
    "        batch_size: size of each batch to training.\n",
    "        print_every: frequency of printing loss (e.g, every 5 epochs)\n",
    "    \n",
    "    Returns:\n",
    "        A list of loss values for each epoch.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    epoch_losses = []\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        permutation = torch.randperm(x_train.size(0))\n",
    "        epoch_loss = 0\n",
    "\n",
    "        # Mini-batch training\n",
    "        for i in range(0, x_train.size(0), batch_size):\n",
    "            # Select mini-batch\n",
    "            index = permutation[i:i+ batch_size]\n",
    "            batch_x, batch_y = x_train[index], y_train[index]\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_x).squeeze()\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss for each epoch\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Average loss for each epoch\n",
    "        avg_epoch_loss = epoch_loss / len(permutation)\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "\n",
    "        # Print progress for each epoch\n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}, Loss: {avg_epoch_loss}]\")\n",
    "    \n",
    "    return model, epoch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20, Loss: 0.004931228272616863]\n",
      "Epoch [10/20, Loss: 0.003983619369566441]\n",
      "Epoch [15/20, Loss: 0.003323741973377764]\n",
      "Epoch [20/20, Loss: 0.0029450216004624965]\n"
     ]
    }
   ],
   "source": [
    "model, losses = train(mlp_model, X_train, y_train, criterion, optimizer, num_epochs=20, batch_size=64, print_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test dataset and print performance metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        model (nn.Module): Trained model to evaluate.\n",
    "        X_test (torch.Tensor): Test features.\n",
    "        y_test (torch.Tensor): Test labels.\n",
    "        \n",
    "    Returns:\n",
    "        Accuracy of the model on the test set.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x_test)\n",
    "        predictions = (outputs > 0.5).int()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "    print('Test Accuracy:', accuracy)\n",
    "    print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.93      0.92       988\n",
      "         1.0       0.93      0.92      0.93      1012\n",
      "\n",
      "    accuracy                           0.93      2000\n",
      "   macro avg       0.93      0.93      0.92      2000\n",
      "weighted avg       0.93      0.93      0.93      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Sentiment for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocessing the new data using process tweet function and Word2Vec model defined above.\n",
    "    \n",
    "    \"\"\"\n",
    "    processed_tokens = process_tweet(text)\n",
    "    \n",
    "    tweet_embedding = get_tweet_embedding(processed_tokens, word2vec_model)\n",
    "    \n",
    "    # Convert to a tensor and reshape to match the model's expected input shape\n",
    "    tweet_embedding_tensor = torch.tensor(tweet_embedding, dtype=torch.float32).unsqueeze(0)\n",
    "    return tweet_embedding_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, text):\n",
    "    \"\"\"\n",
    "    Predict the sentiment of a given text using the trained model.\n",
    "    \n",
    "    Parameters:\n",
    "        model: the trained MLP model.\n",
    "        text: input text to analyze.\n",
    "    \n",
    "    Returns:\n",
    "        \"Positive\" if sentiment is positive, otherwise \"Negative\".\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess the text and get the embedding\n",
    "    input_tensor = preprocess_text(text)\n",
    "    \n",
    "    # Disable gradient computation for inference\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)  # Model outputs probability due to Sigmoid activation\n",
    "    \n",
    "    # Interpret the output\n",
    "    prediction = (output.item() > 0.5)  # Threshold at 0.5 for binary classification\n",
    "    sentiment = \"Positive\" if prediction else \"Negative\"\n",
    "    \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text to analyze\n",
    "new_text = \"Oh great, it's raining again!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "# Predict the sentiment\n",
    "sentiment = predict_sentiment(model, new_text)\n",
    "print(sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
