{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Exercise 2: Neural Network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going from raw text to tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install datasets\n",
    "%pip install tokenizers\n",
    "%pip install torch\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Datasets from Hugging Face could be useful when we create our own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vpming\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "def create_dataset(raw_sentences: list[str]) -> datasets.Dataset:\n",
    "    \"\"\"\n",
    "    Create a HuggingFace Dataset.\n",
    "    \n",
    "    Parameters: \n",
    "        raw_sentences: list of sentences.\n",
    "        labels: list of integer labels corresponding to the sentences.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_dict = {\n",
    "        \"text\": raw_sentences,\n",
    "\n",
    "    }\n",
    "\n",
    "    # Define schema\n",
    "    dataset_features = datasets.Features(\n",
    "        {\n",
    "            \"text\": datasets.Value(\"string\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create the datset\n",
    "    dataset = datasets.Dataset.from_dict(dataset_dict, features=dataset_features)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and extract sentences from book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open('alice_in_wonderland.txt', 'r', encoding='utf-8') as file:\n",
    "    story_text = file.read()\n",
    "\n",
    "raw_sentences = []\n",
    "\n",
    "# Extract Chapter 1\n",
    "chapter_start = \"Alice was beginning\"\n",
    "chapter_end = \"THE END\"\n",
    "start_idx = story_text.find(chapter_start)\n",
    "end_idx = story_text.find(chapter_end)\n",
    "chapter_1_text = story_text[start_idx:end_idx].strip()\n",
    "\n",
    "# Split into sentences\n",
    "# Use regular expressions to split by special signs like '.', '!', and '?'\n",
    "split_sentences = re.split(r'[.!?*;,]', chapter_1_text)\n",
    "\n",
    "# Filter sentences longer than 5 words\n",
    "long_sentences = [sentence.strip() for sentence in split_sentences if len(sentence.split()) > 2]\n",
    "\n",
    "# Display the results\n",
    "for sentence in long_sentences:  \n",
    "    raw_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text\n",
      "0     Alice was beginning to get very tired of sitti...\n",
      "1     and of having nothing to do: once or twice she...\n",
      "2        but it had no pictures or\\nconversations in it\n",
      "3                        “and what is the use of a book\n",
      "4     ” thought Alice\\n“without pictures or conversa...\n",
      "...                                                 ...\n",
      "3568  and make _their_ eyes bright and eager with ma...\n",
      "3569  perhaps even with the dream of Wonderland of l...\n",
      "3570      and find a pleasure in all\\ntheir simple joys\n",
      "3571                     remembering her own child-life\n",
      "3572                         and the happy summer\\ndays\n",
      "\n",
      "[3573 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "own_dataset = create_dataset(raw_sentences)\n",
    "\n",
    "print(own_dataset.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences between WordPiece tokenization and wordLevel tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to train WordLevel Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_level_tokenizer(\n",
    "        sentences: list[str],\n",
    "        unk_token: str = \"[UNK]\",\n",
    "        pad_token: str = \"[PAD]\",\n",
    "        start_of_seq_token: str = \"<s>\",\n",
    "        end_of_seq_token: str = \"</s>\", \n",
    "        vocab_size: int = 1000\n",
    ") -> PreTrainedTokenizerFast:\n",
    "    \"\"\"Train a WordLevel tokenizer.\"\"\"\n",
    "    special_tokens = [unk_token, pad_token, start_of_seq_token, end_of_seq_token]\n",
    "    trainer = trainers.WordLevelTrainer(vocab_size=vocab_size,\n",
    "                                        special_tokens=special_tokens, \n",
    "                                        show_progress=True)\n",
    "\n",
    "    # Initialize WordLevel tokenizer\n",
    "    tokenizer = Tokenizer(models.WordLevel(unk_token=unk_token))\n",
    "\n",
    "    # Normalize each sentence using NFD unicode and stripping whitespace\n",
    "    tokenizer.normalizer = normalizers.Sequence(\n",
    "        [normalizers.NFD(), normalizers.Strip()]\n",
    "    )\n",
    "\n",
    "    # Using Whitespace to split each input sentence\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "    # Post-process for sequence boundaries\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=f\"{start_of_seq_token} $A {end_of_seq_token}\",\n",
    "        special_tokens=[\n",
    "            (start_of_seq_token, special_tokens.index(start_of_seq_token)),\n",
    "            (end_of_seq_token, special_tokens.index(end_of_seq_token)),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train tokenizer\n",
    "    tokenizer.train_from_iterator(sentences, trainer=trainer)\n",
    "\n",
    "    # Enable padding\n",
    "    tokenizer.enable_padding(pad_id=special_tokens.index(pad_token), pad_token=pad_token)\n",
    "\n",
    "    # Wrap in PreTrainedTokenizerFast\n",
    "    pretrained_tokenizer = PreTrainedTokenizerFast(\n",
    "        bos_token=start_of_seq_token,\n",
    "        eos_token=end_of_seq_token,\n",
    "        unk_token=unk_token,\n",
    "        pad_token=pad_token,\n",
    "        tokenizer_object=tokenizer,\n",
    "    )\n",
    "    return pretrained_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to train WordPiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wordpiece_tokenizer(\n",
    "    sentences: list[str],\n",
    "    unk_token: str = \"[UNK]\",\n",
    "    pad_token: str = \"[PAD]\",\n",
    "    start_of_seq_token: str = \"<s>\",\n",
    "    end_of_seq_token: str = \"</s>\",\n",
    "    vocab_size: int = 1000  # Set a smaller vocab size to force subword splits\n",
    ") -> PreTrainedTokenizerFast:\n",
    "    \"\"\"Train a WordPiece tokenizer.\"\"\"\n",
    "    special_tokens = [unk_token, pad_token, start_of_seq_token, end_of_seq_token]\n",
    "    trainer = trainers.WordPieceTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=special_tokens,\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "    # Initialize WordPiece tokenizer\n",
    "    tokenizer = Tokenizer(models.WordPiece(unk_token=unk_token))\n",
    "\n",
    "    # Configure normalization and pre-tokenization\n",
    "    tokenizer.normalizer = normalizers.Sequence([normalizers.NFD(), normalizers.Strip()])\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "    # Post-process for sequence boundaries\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=f\"{start_of_seq_token} $A {end_of_seq_token}\",\n",
    "        special_tokens=[\n",
    "            (start_of_seq_token, special_tokens.index(start_of_seq_token)),\n",
    "            (end_of_seq_token, special_tokens.index(end_of_seq_token)),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train tokenizer\n",
    "    tokenizer.train_from_iterator(sentences, trainer=trainer)\n",
    "\n",
    "    # Enable padding\n",
    "    tokenizer.enable_padding(pad_id=special_tokens.index(pad_token), pad_token=pad_token)\n",
    "\n",
    "    # Wrap in PreTrainedTokenizerFast\n",
    "    pretrained_tokenizer = PreTrainedTokenizerFast(\n",
    "        bos_token=start_of_seq_token,\n",
    "        eos_token=end_of_seq_token,\n",
    "        unk_token=unk_token,\n",
    "        pad_token=pad_token,\n",
    "        tokenizer_object=tokenizer,\n",
    "    )\n",
    "    return pretrained_tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train both tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_level_tokenizer = train_word_level_tokenizer(raw_sentences)\n",
    "wordpiece_tokenizer = train_wordpiece_tokenizer(raw_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize how tokenizers work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: by the way the people near the door began sneezing all at once\n",
      "WordPiece Tokens: ['<s>', 'by', 'the', 'way', 'the', 'people', 'near', 'the', 'door', 'began', 'sneez', '##ing', 'all', 'at', 'once', '</s>']\n",
      "WordPiece Token IDs: [2, 360, 125, 350, 125, 889, 519, 125, 493, 356, 994, 130, 202, 187, 508, 3]\n",
      "\n",
      "WordLevel Tokens: ['<s>', 'by', 'the', 'way', 'the', 'people', 'near', 'the', 'door', 'began', 'sneezing', 'all', 'at', 'once', '</s>']\n",
      "WordLevel Token IDs: [2, 89, 4, 88, 4, 311, 268, 4, 158, 87, 612, 30, 25, 156, 3]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Print tokenized results for raw sentences using convert_ids_to_tokens\n",
    "index = random.randint(0,3572)\n",
    "sentence = raw_sentences[index]\n",
    "\n",
    "wordpiece_encoded_example = wordpiece_tokenizer.encode(sentence)\n",
    "wordpiece_tokens_example = wordpiece_tokenizer.convert_ids_to_tokens(wordpiece_encoded_example)\n",
    "\n",
    "wordlevel_encoded_example = word_level_tokenizer.encode(sentence)\n",
    "wordlevel_tokens_example = word_level_tokenizer.convert_ids_to_tokens(wordlevel_encoded_example)\n",
    "\n",
    "# Using WordPiece Tokenizer\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"WordPiece Tokens: {wordpiece_tokens_example}\")\n",
    "print(f\"WordPiece Token IDs: {wordpiece_encoded_example}\")\n",
    "print()\n",
    "\n",
    "# Using WordLevel Tokenizer\n",
    "print(f\"WordLevel Tokens: {wordlevel_tokens_example}\")\n",
    "print(f\"WordLevel Token IDs: {wordlevel_encoded_example}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RNN model based on different tokenizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fixed maximum length for padding\n",
    "max_length = 20\n",
    "# Tokenize the whole dataset\n",
    "wordpiece_encoded = [wordpiece_tokenizer.encode(sentence, max_length=max_length, padding='max_length', truncation=True) for sentence in raw_sentences]\n",
    "wordlevel_encoded = [word_level_tokenizer.encode(sentence, max_length=max_length, padding='max_length', truncation=True) for sentence in raw_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpiece_data = torch.tensor(wordpiece_encoded, dtype=torch.long)\n",
    "wordlevel_data = torch.tensor(wordlevel_encoded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True,)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        logits = self.fc(output)\n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create Tensor datasets\n",
    "wordpiece_dataset = TensorDataset(wordpiece_data)\n",
    "wordlevel_dataset = TensorDataset(wordlevel_data)\n",
    "\n",
    "# Make sure to shuffle your data to avoid sequence patterns during training\n",
    "wordpiece_loader = DataLoader(wordpiece_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "wordlevel_loader = DataLoader(wordlevel_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "def train_model(model, data_loader, vocab_size, epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in data_loader:\n",
    "            inputs, = batch  \n",
    "            targets = inputs[:, 1:].contiguous()\n",
    "            inputs = inputs[:, :-1].contiguous()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(inputs)\n",
    "            loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(data_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "vocab_size_wordpiece = len(wordpiece_tokenizer.get_vocab())\n",
    "vocab_size_wordlevel = len(word_level_tokenizer.get_vocab())\n",
    "wordpiece_model = RNNLanguageModel(vocab_size_wordpiece, embedding_dim, hidden_dim)\n",
    "wordlevel_model = RNNLanguageModel(vocab_size_wordlevel, embedding_dim, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WordPiece Model:\n",
      "Epoch 1/20, Loss: 3.8572\n",
      "Epoch 2/20, Loss: 3.0665\n",
      "Epoch 3/20, Loss: 2.8793\n",
      "Epoch 4/20, Loss: 2.7321\n",
      "Epoch 5/20, Loss: 2.5960\n",
      "Epoch 6/20, Loss: 2.4772\n",
      "Epoch 7/20, Loss: 2.3693\n",
      "Epoch 8/20, Loss: 2.2760\n",
      "Epoch 9/20, Loss: 2.1966\n",
      "Epoch 10/20, Loss: 2.1259\n",
      "Epoch 11/20, Loss: 2.0644\n",
      "Epoch 12/20, Loss: 2.0061\n",
      "Epoch 13/20, Loss: 1.9519\n",
      "Epoch 14/20, Loss: 1.9032\n",
      "Epoch 15/20, Loss: 1.8577\n",
      "Epoch 16/20, Loss: 1.8127\n",
      "Epoch 17/20, Loss: 1.7718\n",
      "Epoch 18/20, Loss: 1.7317\n",
      "Epoch 19/20, Loss: 1.6935\n",
      "Epoch 20/20, Loss: 1.6536\n"
     ]
    }
   ],
   "source": [
    "# Train model by WordPiece Model\n",
    "print(\"Training WordPiece Model:\")\n",
    "train_model(wordpiece_model, wordpiece_loader, vocab_size_wordpiece, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WordLevel Model:\n",
      "Epoch 1/10, Loss: 3.0236\n",
      "Epoch 2/10, Loss: 2.1691\n",
      "Epoch 3/10, Loss: 2.0183\n",
      "Epoch 4/10, Loss: 1.9143\n",
      "Epoch 5/10, Loss: 1.8363\n",
      "Epoch 6/10, Loss: 1.7729\n",
      "Epoch 7/10, Loss: 1.7179\n",
      "Epoch 8/10, Loss: 1.6706\n",
      "Epoch 9/10, Loss: 1.6286\n",
      "Epoch 10/10, Loss: 1.5915\n"
     ]
    }
   ],
   "source": [
    "# Train model by WordLevel Model\n",
    "print(\"Training WordLevel Model:\")\n",
    "train_model(wordlevel_model, wordlevel_loader, vocab_size_wordlevel, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, start_text, max_length=25, temperature=0.7):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(start_text)\n",
    "    input_tensor = torch.tensor(tokens).unsqueeze(0)\n",
    "    \n",
    "    generated_text = start_text\n",
    "    hidden = None\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        logits, hidden = model(input_tensor, hidden)\n",
    "        \n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        probs[0, tokenizer.pad_token_id] = 0  # Set pad token probability to zero\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        \n",
    "        if next_token == tokenizer.pad_token_id or next_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "        \n",
    "        next_word = tokenizer.decode([next_token])\n",
    "        generated_text += \" \" + next_word\n",
    "        input_tensor = torch.tensor([[next_token]])\n",
    "\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text using WordPiece Model:\n",
      "Alice ##ed s ##ever ##el\n",
      "\n",
      "Generated Text using WordLevel Model:\n",
      "Alice shoes a very [UNK]\n"
     ]
    }
   ],
   "source": [
    "start_text = \"Alice\"\n",
    "print(\"Generated Text using WordPiece Model:\")\n",
    "print(generate_text(wordpiece_model, wordpiece_tokenizer, start_text))\n",
    "\n",
    "print(\"\\nGenerated Text using WordLevel Model:\")\n",
    "print(generate_text(wordlevel_model, word_level_tokenizer, start_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "# Split into training and validation set\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "val_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# Visualize the dataset\n",
    "dataset_df = dataset.to_pandas()\n",
    "print(dataset_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a pre-trained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "padding_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Example for using tokenizer with padding\n",
    "sequence = [\"I am a student at Tampere University\", \"I live in Finland\"]\n",
    "model_inputs = tokenizer(sequence, padding='longest', truncation=True) # try with padding = 'max_length'\n",
    "print(model_inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and prepare data for language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The 'labels' are shifted version of 'input_ids', meaning each token in 'labels' corresponds to the next word in 'input_ids'.\n",
    "- The model's prediction at position 'i' in 'input_ids' should match the word at position 'i+1' in 'labels'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_shift(examples):\n",
    "    # Tokenize with padding and truncation\n",
    "    tokenized = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    \n",
    "    # Copy input_ids for labels and then shift for next-token prediction\n",
    "    labels = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    # Shift the labels one position to the right\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = labels[i][1:] + [padding_token_id]  # Shift and pad\n",
    "    \n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping the function to the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to both datasets\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_and_shift, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_and_shift, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Set the format for PyTorch\n",
    "tokenized_train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"labels\"])\n",
    "tokenized_val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(tokenized_train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(tokenized_val_dataset, batch_size=8, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Language Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, padding_token_id: int):\n",
    "        super().__init__()\n",
    "        self.padding_token_id = padding_token_id\n",
    "\n",
    "        # Model components\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.projection = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        # Loss function, ignoring padding tokens\n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=padding_token_id)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        logits = self.predict_logits(input_ids)\n",
    "        loss = self.compute_loss(logits, input_ids)\n",
    "        return loss, logits\n",
    "\n",
    "    def predict_logits(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Processes the embeddings sequentially, allowing each token to influence the representation\n",
    "        of the subsequent tokens. This helps capture dependencies accross the sequence.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): The embeddings tensors from the \n",
    "            previous step with shape [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor with shape [batch_size, seq_len, hidden_dim], \n",
    "            where hidden_dim is the dimensionality of the RNN's hidden state.\n",
    "\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        rnn_output, _ = self.rnn(embeddings)  \n",
    "        logits = self.projection(rnn_output)\n",
    "        return logits\n",
    "\n",
    "    def compute_loss(self, logits: torch.Tensor, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        logits = logits[:, :-1, :].contiguous().view(-1, logits.size(-1))\n",
    "        target_ids = input_ids[:, 1:].contiguous().view(-1)\n",
    "        loss = self.loss_fn(logits, target_ids)\n",
    "        return loss\n",
    "\n",
    "    def generate_text(self, prompt, max_len=20, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generates text from a model, starting with a given prompt and\n",
    "        extending it one token at a time until reach a specified length.\n",
    "\n",
    "        Args:\n",
    "            prompt (_type_): Prompt.\n",
    "            max_len (int, optional): The maximum length of tokens to generate.\n",
    "            temperature (float, optional):  A parameter that controls the randomness of predictions. \n",
    "                                        Lower values make the model more confident and deterministic, \n",
    "                                        while higher values make it more diverse and exploratory..\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        # Switch to Evaluation mode\n",
    "        self.eval()\n",
    "        # Convert prompts to tensor\n",
    "        input_ids = torch.tensor([prompt], dtype=torch.long)\n",
    "        generated_tokens = prompt[:]\n",
    "        \n",
    "        # Disable Gradient computation\n",
    "        with torch.no_grad():\n",
    "            # Generate tokens in a loop\n",
    "            for _ in range(max_len):\n",
    "                logits = self.predict_logits(input_ids)  \n",
    "                next_token_logits = logits[:, -1, :] / temperature # extracts the logits for the last token\n",
    "                \n",
    "                # Convert logits to probabilities\n",
    "                probabilities = F.softmax(next_token_logits, dim=-1).squeeze()\n",
    "                # Uses torch.multinomial to sample a token ID from the probability distribution.\n",
    "                next_token = torch.multinomial(probabilities, 1).item()\n",
    "                \n",
    "                # End condition\n",
    "                if next_token == self.padding_token_id:\n",
    "                    break\n",
    "\n",
    "                generated_tokens.append(next_token)\n",
    "                input_ids = torch.cat([input_ids, torch.tensor([[next_token]])], dim=1)\n",
    "\n",
    "        # Conver tokens to text\n",
    "        generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model, optimizer, and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "epochs = 3\n",
    "\n",
    "model = LanguageModel(vocab_size, embedding_dim, hidden_dim, padding_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to evalulate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_batches = len(dataloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            loss, _ = model(input_ids)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss and perplexity\n",
    "    avg_loss = total_loss / total_batches\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, _ = model(input_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Run evaluation\n",
    "    val_loss, val_perplexity = evaluate_model(model, val_loader)\n",
    "    print(f\"Epoch {epoch+1} - Validation Loss: {val_loss:.4f}, Perplexity: {val_perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text from a promt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"\"\n",
    "prompt_tokens = tokenizer.encode(prompt_text, return_tensors=\"pt\").squeeze().tolist()\n",
    "\n",
    "generated_text = model.generate_text(prompt_tokens, max_len=50, temperature=0.8)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation using transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='distilgpt2')\n",
    "generator(\n",
    "    \"In this course, we will teach you about\",\n",
    "    max_length=30,\n",
    "    truncation=True,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=generator.tokenizer.eos_token_id\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
