{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Exercise 2: Neural Network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going from raw text to tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install datasets\n",
    "%pip install tokenizers\n",
    "%pip install torch\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Datasets from Hugging Face could be useful when we create our own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vpming\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "def create_dataset(raw_sentences: list[str]) -> datasets.Dataset:\n",
    "    \"\"\"\n",
    "    Create a HuggingFace Dataset.\n",
    "    \n",
    "    Parameters: \n",
    "        raw_sentences: list of sentences.\n",
    "        labels: list of integer labels corresponding to the sentences.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_dict = {\n",
    "        \"text\": raw_sentences,\n",
    "\n",
    "    }\n",
    "\n",
    "    # Define schema\n",
    "    dataset_features = datasets.Features(\n",
    "        {\n",
    "            \"text\": datasets.Value(\"string\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create the datset\n",
    "    dataset = datasets.Dataset.from_dict(dataset_dict, features=dataset_features)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and extract sentences from book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open('alice_in_wonderland.txt', 'r', encoding='utf-8') as file:\n",
    "    story_text = file.read()\n",
    "\n",
    "raw_sentences = []\n",
    "\n",
    "# Extract Chapter 1\n",
    "chapter_start = \"Alice was beginning\"\n",
    "chapter_end = \"THE END\"\n",
    "start_idx = story_text.find(chapter_start)\n",
    "end_idx = story_text.find(chapter_end)\n",
    "chapter_1_text = story_text[start_idx:end_idx].strip()\n",
    "\n",
    "# Split into sentences\n",
    "# Use regular expressions to split by special signs like '.', '!', and '?'\n",
    "split_sentences = re.split(r'[.!?*;,]', chapter_1_text)\n",
    "\n",
    "# Filter sentences longer than 5 words\n",
    "long_sentences = [sentence.strip() for sentence in split_sentences if len(sentence.split()) > 2]\n",
    "\n",
    "# Display the results\n",
    "for sentence in long_sentences:  \n",
    "    raw_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2681"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_distinct_words(text):\n",
    "    # Create an empty set to store unique words\n",
    "    unique_words = set()\n",
    "\n",
    "    # Split text into words using regular expressions\n",
    "    words = re.split(r'\\W+', text.lower())  # This splits at any non-alphanumeric character\n",
    "\n",
    "    # Add each word to the set\n",
    "    for word in words:\n",
    "        if word:  # This check avoids adding empty strings\n",
    "            unique_words.add(word)\n",
    "\n",
    "    # Return the number of distinct words\n",
    "    return len(unique_words)\n",
    "count_distinct_words(chapter_1_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text\n",
      "0     Alice was beginning to get very tired of sitti...\n",
      "1     and of having nothing to do: once or twice she...\n",
      "2        but it had no pictures or\\nconversations in it\n",
      "3                        “and what is the use of a book\n",
      "4     ” thought Alice\\n“without pictures or conversa...\n",
      "...                                                 ...\n",
      "3568  and make _their_ eyes bright and eager with ma...\n",
      "3569  perhaps even with the dream of Wonderland of l...\n",
      "3570      and find a pleasure in all\\ntheir simple joys\n",
      "3571                     remembering her own child-life\n",
      "3572                         and the happy summer\\ndays\n",
      "\n",
      "[3573 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "own_dataset = create_dataset(raw_sentences)\n",
    "\n",
    "print(own_dataset.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences between WordPiece tokenization and wordLevel tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to train WordLevel Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_level_tokenizer(\n",
    "        sentences: list[str],\n",
    "        unk_token: str = \"[UNK]\",\n",
    "        pad_token: str = \"[PAD]\",\n",
    "        start_of_seq_token: str = \"<s>\",\n",
    "        end_of_seq_token: str = \"</s>\", \n",
    "        vocab_size: int = 3000\n",
    ") -> PreTrainedTokenizerFast:\n",
    "    \"\"\"Train a WordLevel tokenizer.\"\"\"\n",
    "    special_tokens = [unk_token, pad_token, start_of_seq_token, end_of_seq_token]\n",
    "    trainer = trainers.WordLevelTrainer(vocab_size=vocab_size,\n",
    "                                        special_tokens=special_tokens, \n",
    "                                        show_progress=True)\n",
    "\n",
    "    # Initialize WordLevel tokenizer\n",
    "    tokenizer = Tokenizer(models.WordLevel(unk_token=unk_token))\n",
    "\n",
    "    # Normalize each sentence using NFD unicode and stripping whitespace\n",
    "    tokenizer.normalizer = normalizers.Sequence(\n",
    "        [normalizers.NFD(), normalizers.Strip()]\n",
    "    )\n",
    "\n",
    "    # Using Whitespace to split each input sentence\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "    # Post-process for sequence boundaries\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=f\"{start_of_seq_token} $A {end_of_seq_token}\",\n",
    "        special_tokens=[\n",
    "            (start_of_seq_token, special_tokens.index(start_of_seq_token)),\n",
    "            (end_of_seq_token, special_tokens.index(end_of_seq_token)),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train tokenizer\n",
    "    tokenizer.train_from_iterator(sentences, trainer=trainer)\n",
    "\n",
    "    # Enable padding\n",
    "    tokenizer.enable_padding(pad_id=special_tokens.index(pad_token), pad_token=pad_token)\n",
    "\n",
    "    # Wrap in PreTrainedTokenizerFast\n",
    "    pretrained_tokenizer = PreTrainedTokenizerFast(\n",
    "        bos_token=start_of_seq_token,\n",
    "        eos_token=end_of_seq_token,\n",
    "        unk_token=unk_token,\n",
    "        pad_token=pad_token,\n",
    "        tokenizer_object=tokenizer,\n",
    "    )\n",
    "    return pretrained_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to train WordPiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wordpiece_tokenizer(\n",
    "    sentences: list[str],\n",
    "    unk_token: str = \"[UNK]\",\n",
    "    pad_token: str = \"[PAD]\",\n",
    "    start_of_seq_token: str = \"<s>\",\n",
    "    end_of_seq_token: str = \"</s>\",\n",
    "    vocab_size: int = 3000  # Set a smaller vocab size to force subword splits\n",
    ") -> PreTrainedTokenizerFast:\n",
    "    \"\"\"Train a WordPiece tokenizer.\"\"\"\n",
    "    special_tokens = [unk_token, pad_token, start_of_seq_token, end_of_seq_token]\n",
    "    trainer = trainers.WordPieceTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=special_tokens,\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "    # Initialize WordPiece tokenizer\n",
    "    tokenizer = Tokenizer(models.WordPiece(unk_token=unk_token))\n",
    "\n",
    "    # Configure normalization and pre-tokenization\n",
    "    tokenizer.normalizer = normalizers.Sequence([normalizers.NFD(), normalizers.Strip()])\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "    # Post-process for sequence boundaries\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=f\"{start_of_seq_token} $A {end_of_seq_token}\",\n",
    "        special_tokens=[\n",
    "            (start_of_seq_token, special_tokens.index(start_of_seq_token)),\n",
    "            (end_of_seq_token, special_tokens.index(end_of_seq_token)),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train tokenizer\n",
    "    tokenizer.train_from_iterator(sentences, trainer=trainer)\n",
    "\n",
    "    # Enable padding\n",
    "    tokenizer.enable_padding(pad_id=special_tokens.index(pad_token), pad_token=pad_token)\n",
    "\n",
    "    # Wrap in PreTrainedTokenizerFast\n",
    "    pretrained_tokenizer = PreTrainedTokenizerFast(\n",
    "        bos_token=start_of_seq_token,\n",
    "        eos_token=end_of_seq_token,\n",
    "        unk_token=unk_token,\n",
    "        pad_token=pad_token,\n",
    "        tokenizer_object=tokenizer,\n",
    "    )\n",
    "    return pretrained_tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train both tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_level_tokenizer = train_word_level_tokenizer(raw_sentences)\n",
    "wordpiece_tokenizer = train_wordpiece_tokenizer(raw_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize how tokenizers work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ” added the Dormouse\n",
      "WordPiece Tokens: ['<s>', '”', 'added', 'the', 'Dormouse', '</s>']\n",
      "WordPiece Token IDs: [2, 68, 607, 125, 449, 3]\n",
      "\n",
      "WordLevel Tokens: ['<s>', '”', 'added', 'the', 'Dormouse', '</s>']\n",
      "WordLevel Token IDs: [2, 5, 191, 4, 129, 3]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Print tokenized results for raw sentences using convert_ids_to_tokens\n",
    "index = random.randint(0,3572)\n",
    "sentence = raw_sentences[index]\n",
    "\n",
    "wordpiece_encoded_example = wordpiece_tokenizer.encode(sentence)\n",
    "wordpiece_tokens_example = wordpiece_tokenizer.convert_ids_to_tokens(wordpiece_encoded_example)\n",
    "\n",
    "wordlevel_encoded_example = word_level_tokenizer.encode(sentence)\n",
    "wordlevel_tokens_example = word_level_tokenizer.convert_ids_to_tokens(wordlevel_encoded_example)\n",
    "\n",
    "# Using WordPiece Tokenizer\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"WordPiece Tokens: {wordpiece_tokens_example}\")\n",
    "print(f\"WordPiece Token IDs: {wordpiece_encoded_example}\")\n",
    "print()\n",
    "\n",
    "# Using WordLevel Tokenizer\n",
    "print(f\"WordLevel Tokens: {wordlevel_tokens_example}\")\n",
    "print(f\"WordLevel Token IDs: {wordlevel_encoded_example}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RNN model based on different tokenizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fixed maximum length for padding\n",
    "max_length = 50\n",
    "# Tokenize the whole dataset\n",
    "wordpiece_encoded = [wordpiece_tokenizer.encode(sentence, max_length=max_length, padding='max_length', truncation=True) for sentence in raw_sentences]\n",
    "wordlevel_encoded = [word_level_tokenizer.encode(sentence, max_length=max_length, padding='max_length', truncation=True) for sentence in raw_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpiece_data = torch.tensor(wordpiece_encoded, dtype=torch.long)\n",
    "wordlevel_data = torch.tensor(wordlevel_encoded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        logits = self.fc(output)\n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create Tensor datasets\n",
    "wordpiece_dataset = TensorDataset(wordpiece_data)\n",
    "wordlevel_dataset = TensorDataset(wordlevel_data)\n",
    "\n",
    "# Shuffle your data to avoid sequence patterns during training\n",
    "wordpiece_loader = DataLoader(wordpiece_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "wordlevel_loader = DataLoader(wordlevel_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "def train_model(model, data_loader, vocab_size, epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in data_loader:\n",
    "            inputs, = batch  \n",
    "            targets = inputs[:, 1:].contiguous()\n",
    "            inputs = inputs[:, :-1].contiguous()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(inputs)\n",
    "            loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(data_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "vocab_size_wordpiece = len(wordpiece_tokenizer.get_vocab())\n",
    "vocab_size_wordlevel = len(word_level_tokenizer.get_vocab())\n",
    "wordpiece_model = RNNLanguageModel(vocab_size_wordpiece, embedding_dim, hidden_dim)\n",
    "wordlevel_model = RNNLanguageModel(vocab_size_wordlevel, embedding_dim, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WordPiece Model:\n",
      "Epoch 1/20, Loss: 2.0067\n",
      "Epoch 2/20, Loss: 1.1305\n",
      "Epoch 3/20, Loss: 1.0554\n",
      "Epoch 4/20, Loss: 1.0057\n",
      "Epoch 5/20, Loss: 0.9671\n",
      "Epoch 6/20, Loss: 0.9338\n",
      "Epoch 7/20, Loss: 0.9033\n",
      "Epoch 8/20, Loss: 0.8770\n",
      "Epoch 9/20, Loss: 0.8541\n",
      "Epoch 10/20, Loss: 0.8323\n",
      "Epoch 11/20, Loss: 0.8117\n",
      "Epoch 12/20, Loss: 0.7928\n",
      "Epoch 13/20, Loss: 0.7743\n",
      "Epoch 14/20, Loss: 0.7566\n",
      "Epoch 15/20, Loss: 0.7399\n",
      "Epoch 16/20, Loss: 0.7226\n",
      "Epoch 17/20, Loss: 0.7074\n",
      "Epoch 18/20, Loss: 0.6908\n",
      "Epoch 19/20, Loss: 0.6754\n",
      "Epoch 20/20, Loss: 0.6603\n"
     ]
    }
   ],
   "source": [
    "# Train model by WordPiece Model\n",
    "print(\"Training WordPiece Model:\")\n",
    "train_model(wordpiece_model, wordpiece_loader, vocab_size_wordpiece, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WordLevel Model:\n",
      "Epoch 1/20, Loss: 1.8892\n",
      "Epoch 2/20, Loss: 1.0364\n",
      "Epoch 3/20, Loss: 0.9653\n",
      "Epoch 4/20, Loss: 0.9195\n",
      "Epoch 5/20, Loss: 0.8816\n",
      "Epoch 6/20, Loss: 0.8488\n",
      "Epoch 7/20, Loss: 0.8194\n",
      "Epoch 8/20, Loss: 0.7933\n",
      "Epoch 9/20, Loss: 0.7697\n",
      "Epoch 10/20, Loss: 0.7496\n",
      "Epoch 11/20, Loss: 0.7301\n",
      "Epoch 12/20, Loss: 0.7128\n",
      "Epoch 13/20, Loss: 0.6956\n",
      "Epoch 14/20, Loss: 0.6795\n",
      "Epoch 15/20, Loss: 0.6644\n",
      "Epoch 16/20, Loss: 0.6496\n",
      "Epoch 17/20, Loss: 0.6343\n",
      "Epoch 18/20, Loss: 0.6205\n",
      "Epoch 19/20, Loss: 0.6067\n",
      "Epoch 20/20, Loss: 0.5927\n"
     ]
    }
   ],
   "source": [
    "# Train model by WordLevel Model\n",
    "print(\"Training WordLevel Model:\")\n",
    "train_model(wordlevel_model, wordlevel_loader, vocab_size_wordlevel, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_wordpiece_output(tokens):\n",
    "    \"\"\"\n",
    "    Enhanced cleanup for WordPiece tokens, handling initial '##' tokens correctly.\n",
    "    \"\"\"\n",
    "    cleaned_text = \"\"\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.startswith(\"##\"):\n",
    "            if i == 0 or cleaned_text == \"\":\n",
    "                # If '##' token is the first one, treat as new word without '##'\n",
    "                cleaned_text += token[2:]\n",
    "            else:\n",
    "                # Otherwise, merge directly with the previous token\n",
    "                cleaned_text += token[2:]\n",
    "        else:\n",
    "            # Add space only if it's not the first token in cleaned_text\n",
    "            cleaned_text += (\" \" + token if cleaned_text else token)\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, start_text, max_length=100, temperature=0.7):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(start_text)\n",
    "    input_tensor = torch.tensor(tokens).unsqueeze(0)\n",
    "\n",
    "    generated_text = start_text\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        logits, hidden = model(input_tensor, hidden)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        probs[0, tokenizer.pad_token_id] = 0  # Prevent PAD from being chosen\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        if next_token == tokenizer.pad_token_id or next_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        next_word = tokenizer.decode([next_token])\n",
    "        if tokenizer == wordpiece_tokenizer:\n",
    "            next_word = clean_wordpiece_output([next_word])  # Apply enhanced cleaning function\n",
    "\n",
    "        generated_text += \" \" + next_word\n",
    "        input_tensor = torch.tensor([[next_token]])\n",
    "\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text using WordPiece Model:\n",
      "Alice like at last\n",
      "\n",
      "Generated Text using WordLevel Model:\n",
      "Alice Alice were talking\n"
     ]
    }
   ],
   "source": [
    "start_text = \"Alice\"\n",
    "print(\"Generated Text using WordPiece Model:\")\n",
    "print(generate_text(wordpiece_model, wordpiece_tokenizer, start_text))\n",
    "\n",
    "print(\"\\nGenerated Text using WordLevel Model:\")\n",
    "print(generate_text(wordlevel_model, word_level_tokenizer, start_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  Alice was beginning to get very tired of sitti...\n",
      "1  and of having nothing to do: once or twice she...\n",
      "2     but it had no pictures or\\nconversations in it\n",
      "3                     “and what is the use of a book\n",
      "4  ” thought Alice\\n“without pictures or conversa...\n"
     ]
    }
   ],
   "source": [
    "dataset = own_dataset\n",
    "\n",
    "# Split into training and validation set\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# Visualize the dataset\n",
    "dataset_df = dataset.to_pandas()\n",
    "print(dataset_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a pre-trained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "padding_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and prepare data for language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The 'labels' are shifted version of 'input_ids', meaning each token in 'labels' corresponds to the next word in 'input_ids'.\n",
    "- The model's prediction at position 'i' in 'input_ids' should match the word at position 'i+1' in 'labels'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_shift(examples):\n",
    "    # Tokenize with padding and truncation\n",
    "    tokenized = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    \n",
    "    # Copy input_ids for labels and then shift for next-token prediction\n",
    "    labels = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    # Shift the labels one position to the right\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = labels[i][1:] + [padding_token_id]  # Shift and pad\n",
    "    \n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping the function to the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2858/2858 [00:00<00:00, 10183.39 examples/s]\n",
      "Map: 100%|██████████| 715/715 [00:00<00:00, 13730.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to both datasets\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_and_shift, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_val_dataset = test_dataset.map(tokenize_and_shift, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Set the format for PyTorch\n",
    "tokenized_train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"labels\"])\n",
    "tokenized_val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(tokenized_train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(tokenized_val_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Language Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, padding_token_id: int):\n",
    "        super().__init__()\n",
    "        self.padding_token_id = padding_token_id\n",
    "\n",
    "        # Model components\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.projection = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        # Loss function, ignoring padding tokens\n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=padding_token_id)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        logits = self.predict_logits(input_ids)\n",
    "        loss = self.compute_loss(logits, input_ids)\n",
    "        return loss, logits\n",
    "\n",
    "    def predict_logits(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Processes the embeddings sequentially, allowing each token to influence the representation\n",
    "        of the subsequent tokens. This helps capture dependencies accross the sequence.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): The embeddings tensors from the \n",
    "            previous step with shape [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor with shape [batch_size, seq_len, hidden_dim], \n",
    "            where hidden_dim is the dimensionality of the RNN's hidden state.\n",
    "\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        rnn_output, _ = self.rnn(embeddings)  \n",
    "        logits = self.projection(rnn_output)\n",
    "        return logits\n",
    "\n",
    "    def compute_loss(self, logits: torch.Tensor, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        logits = logits[:, :-1, :].contiguous().view(-1, logits.size(-1))\n",
    "        target_ids = input_ids[:, 1:].contiguous().view(-1)\n",
    "        loss = self.loss_fn(logits, target_ids)\n",
    "        return loss\n",
    "\n",
    "    def generate_text(self, prompt, max_len=20, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generates text from a model, starting with a given prompt and\n",
    "        extending it one token at a time until reach a specified length.\n",
    "\n",
    "        Args:\n",
    "            prompt (_type_): Prompt.\n",
    "            max_len (int, optional): The maximum length of tokens to generate.\n",
    "            temperature (float, optional):  A parameter that controls the randomness of predictions. \n",
    "                                        Lower values make the model more confident and deterministic, \n",
    "                                        while higher values make it more diverse and exploratory..\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        # Switch to Evaluation mode\n",
    "        self.eval()\n",
    "        # Convert prompts to tensor\n",
    "        input_ids = torch.tensor([prompt], dtype=torch.long)\n",
    "        generated_tokens = prompt[:]\n",
    "        \n",
    "        # Disable Gradient computation\n",
    "        with torch.no_grad():\n",
    "            # Generate tokens in a loop\n",
    "            for _ in range(max_len):\n",
    "                logits = self.predict_logits(input_ids)  \n",
    "                next_token_logits = logits[:, -1, :] / temperature # extracts the logits for the last token\n",
    "                \n",
    "                # Convert logits to probabilities\n",
    "                probabilities = F.softmax(next_token_logits, dim=-1).squeeze()\n",
    "                # Uses torch.multinomial to sample a token ID from the probability distribution.\n",
    "                next_token = torch.multinomial(probabilities, 1).item()\n",
    "                \n",
    "                # End condition\n",
    "                if next_token == self.padding_token_id:\n",
    "                    break\n",
    "\n",
    "                generated_tokens.append(next_token)\n",
    "                input_ids = torch.cat([input_ids, torch.tensor([[next_token]])], dim=1)\n",
    "\n",
    "        # Conver tokens to text\n",
    "        generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model, optimizer, and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "epochs = 20\n",
    "\n",
    "model = LanguageModel(vocab_size, embedding_dim, hidden_dim, padding_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to evalulate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_batches = len(dataloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            loss, _ = model(input_ids)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss and perplexity\n",
    "    avg_loss = total_loss / total_batches\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 90/90 [01:06<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 7.2321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:10<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Validation Loss: 5.7756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 90/90 [01:06<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 5.4902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:15<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Validation Loss: 5.5026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 90/90 [01:08<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 5.2557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:10<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Validation Loss: 5.3292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 90/90 [01:07<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training Loss: 5.0675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:09<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Validation Loss: 5.1717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 90/90 [01:06<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training Loss: 4.9100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:10<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Validation Loss: 5.0607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 90/90 [01:05<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training Loss: 4.7718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:09<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Validation Loss: 4.9507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 90/90 [01:04<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training Loss: 4.6475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:09<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Validation Loss: 4.8591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 90/90 [01:03<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training Loss: 4.5352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:09<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Validation Loss: 4.7859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 90/90 [01:04<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training Loss: 4.4450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:10<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Validation Loss: 4.7243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 90/90 [01:06<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Training Loss: 4.3472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:09<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Validation Loss: 4.6653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 90/90 [01:07<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Training Loss: 4.2635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:09<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Validation Loss: 4.6166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 90/90 [01:06<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Training Loss: 4.1817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:11<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Validation Loss: 4.5791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 90/90 [01:07<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Training Loss: 4.1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:15<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Validation Loss: 4.5342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 90/90 [01:06<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Training Loss: 4.0441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:11<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Validation Loss: 4.5059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 90/90 [01:06<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Training Loss: 3.9806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:11<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Validation Loss: 4.4776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|██████████| 90/90 [01:06<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Training Loss: 3.9242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:11<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Validation Loss: 4.4533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|██████████| 90/90 [01:06<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Training Loss: 3.8552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:11<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Validation Loss: 4.4337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|██████████| 90/90 [01:05<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Training Loss: 3.8035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:11<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Validation Loss: 4.4108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|██████████| 90/90 [01:06<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Training Loss: 3.7471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:11<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Validation Loss: 4.3986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20: 100%|██████████| 90/90 [01:06<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Training Loss: 3.6950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 23/23 [00:11<00:00,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Validation Loss: 4.3813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, _ = model(input_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Run evaluation\n",
    "    val_loss = evaluate_model(model, val_loader)\n",
    "    print(f\"Epoch {epoch+1} - Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text from a promt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: the rabbit notice her face _ that\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"The rabbit\"\n",
    "prompt_tokens = tokenizer.encode(prompt_text, return_tensors=\"pt\").squeeze().tolist()\n",
    "\n",
    "generated_text = model.generate_text(prompt_tokens, max_len=10, temperature=0.8)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
